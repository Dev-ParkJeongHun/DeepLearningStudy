{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR_10 데이타셋을 이용한 CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 데이터 셋 \n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "- 총 10개의 클래스    참고) 총 100개의 클래스  CIFAT-100 데이타셋\n",
    "- 총 50,000개의 학습 데이타, 10,000개의 테스트 데이터\n",
    "- 이미지 - 32X32X3 \n",
    "- MNIST 데이터 셋에 비해 데이터 복잡도가 훨씬 높아 높은 성능을 기대하기는 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/cifar-10.jpg\" align=left width=400 height=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경고 무시\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. CIFAR-10 데이타셋을 이용한 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이타 다운로드 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) CIFAR-10 데이터 다운로드 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CIFAR-10 데이터 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat',\n",
    "              'deer', 'dog', 'frog', 'horse',\n",
    "              'ship', 'truck']\n",
    "\n",
    "sample_size = 9\n",
    "random_idx = np.random.randint(60000, size=sample_size) \n",
    "\n",
    "plt.figure(figsize = (5, 5))\n",
    "for i, idx in enumerate(random_idx):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(x_train[i], cmap = 'gray')\n",
    "    plt.xlabel(class_names[int(y_train[i])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이타 셋 전처리 - 피처 표준화\n",
    "- 데이타 셋의 평균과 표준편차를 채널별로 구해 표준화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균과 표준편차는 채널별로 구해줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 검증 데이타 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CNN 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 구성 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pydot-ng\n",
    "# pip install graphviz\n",
    "# Graphviz 패키지 설치, 윈도우에서 환경변수에 경로 추가\n",
    "\n",
    "# import os\n",
    "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 컴파일 설정\n",
    "- 레이블 형태를 원-핫 인코딩을 이용하여 범주형 형태로 변환했을 때 사용하는 손실 함수\n",
    "    - ->  'categorical_crossentropy' 사용\n",
    "- 레이블 형태를 원-핫 인코딩 하지 않고 레이블 형태(0~9) 그대로 사용할 때 사용하는 손실함수 \n",
    "    - -> 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 학습 과정 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 과대적합의 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2. 과대 적합 피하기\n",
    "- 과대적합을 피하기 위한 가장 좋은 방법은 데이터를 충분히 모아놓고 모델을 구현하는 것\n",
    "\n",
    "### 과대 적합 피하는 방법\n",
    "#### (1) 드롭 아웃(drop out) \n",
    "- 과대 적합을 피하기 위해 사용되는 대표적인 방법 중 하나\n",
    "- 학습이 진행되는 동안 신경망의 일부 유닛을 제외(드롭) 한다.\n",
    "- 신경망 모델은 드룹 아웃으로 제외한 유닛 대신 제외하지 않은 유닛을 집중적으로 학습\n",
    "- 과대 적합 문제를 방지함과 동시에 더 나은 성능을 기대할 수 있다. -> 선택과 집중\n",
    "- 드롭 아웃 비율(dropout rate)는 일반적으로 0.2 ~ 0.5를 사용\n",
    "- 드롭아웃은 학습 속도를 느리게 하는 단점이 존재\n",
    "- 테스트 시에는 드룹아웃이 작동하지 않으며, 모든 유닛이 활성화되어 출력하게 된다.\n",
    "\n",
    "#### (2) 규제화 함수(regulaizer) 사용하기\n",
    "- 규제화 함수(regulaizer : 임의로 모델의 복잡도를 제한시키는 것을 의미\n",
    "- L1 노름(라쏘, LASSO), L2 노름(릿지, Ridge), L1 노름과 L2 노름을 혼합한 엘라스틱넷(ElasticNet)\n",
    "- tensorflow, keras, regularizers, l1(l=0.01) - 가중치의 절대값 합\n",
    "- tensorflow, keras, regularizers, l2(l=0.01) - 가중치의 제곱합\n",
    "- tensorflow, keras, regularizers, l1_l2(i1-0.01, l2=0.01) - (혼합) 절대값의 합 + 제곱합\n",
    "- 각 규제화 함수는 기능에 맞게 가중치의 합을 구하여 손실함수에 더하게 된다.\n",
    "- (예) l2 규제화 함수 = 가중치의 제곱합 + 손실 \n",
    "\n",
    "#### (3) 배치 정규화(Batch Normalization)\n",
    "- 근본적으로 과대적합을 피하기 위한 방법은 아님\n",
    "- 배치 정규화와 드롭아웃과 배교하여 설명\n",
    "- 내부 공선성을 해결하기 위해 고안\n",
    "- 신경망의 출력값은 다양한 입력 데이타에 따라 쉽게 변할 수 있는데, \n",
    "- 매우 큰 범위의 출력값은 신경망을 불안정하게 하여 성능저하를 일으킬 수 있다.\n",
    "- 배치 정규화는 출력값이 가질 수 있는 범위,즉 출력값 분포의 범위를 줄여주어 불확실성을 어느 정도 감소시키는 방법\n",
    "- 배치 정규화의 장점\n",
    "    - 1) 기존 신경망은 높은 학습률을 사용하는 경우, 그래디언트 손실/폭발의 문제점이 존재\n",
    "        - 배치 정규화를 사용하면, 이러한 문제를 방지할 수 있어 높은 학습률을 사용하여 빠른 속도로 학습을 진행할 있게 한다.\n",
    "    - 2) 배치 정규화는 자체적인 규제 효과가 있기 때문에, 과대적합 문제를 피할 수 있게 한다.\n",
    "        - 과대적합에 도움이 될 뿐 보장하지는 않는다. \n",
    "        - \"배치정규화를 사용하면 별도의 규제화 함수나 드롭아웃을 사용하지 않아도 된다\"라는 의견이 다수    \n",
    "- 배치 정규화를 사용할 경우, 일반적으로 아래와 같은 순서를 이용하여 모델을 구성\n",
    "    - Dense층 또는 Conv2D층 -> BatchNormalization() -> Activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 드롭 아웃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 드롭아웃을 이용한 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (32, 32, 3)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2)) # 드롭아웃을 추가합니다.\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2)) # 드롭아웃을 추가합니다.\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2)) # 드롭아웃을 추가합니다.\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = Adam(1e-4),\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 30,\n",
    "                    batch_size = 32,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 두 그래프의 선이 벌어지지 않고 있다.\n",
    "- 과대적합을 해결할 방법을 사용하지 않은 모델과 규제화 함수를 사용한 모델과 비교했을 때 \n",
    "- 과대적합을 방지하기에 드롭아웃 방법은 매우 강력해 보임\n",
    "- 드롭아웃은 학습 속도를 느리게 하는 단점이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 규제화 함수 사용\n",
    "- l2 규제화 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 입력 데이터는 (32, 32, 3)의 형태를 가집니다.\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', input_shape = (32, 32, 3)))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = l2(0.001)))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = l2(0.001)))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same', activation = 'relu', kernel_regularizer = l2(0.001)))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = 'relu', kernel_regularizer = l2(0.001)))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = Adam(1e-4),\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 30,\n",
    "                    batch_size = 32,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 규제화 함수를 사용하지 않은 모델에 비해 비교적 그래프가 안정적으로 그려지는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 배치 정규화\n",
    "- 배치 정규화를 사용할 경우, 일반적으로 아래와 같은 순서를 이용하여 모델을 구성\n",
    "    - Dense층 또는 Conv2D층 -> BatchNormalization() -> Activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 배치 정규화 이용한 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Activation, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same', input_shape = (32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(filters = 128, kernel_size = 3, padding = 'same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = Adam(1e-4),\n",
    "             loss = 'sparse_categorical_crossentropy',\n",
    "             metrics = ['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs = 30,\n",
    "                    batch_size = 32,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 10 에폭 이후로 과대 적합이 발생하고 있지만, 가장 높은 성능을 달성했다는 점에서 이를 보완 \n",
    "- 배치 정규화는 다수의 모델에서 사용되고 있기 때문에 자주 접하고 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
