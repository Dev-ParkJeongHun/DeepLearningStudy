{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR_10 데이타셋을 이용한 CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 데이터 셋 \n",
    "- https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "- 총 10개의 클래스    참고) 총 100개의 클래스  CIFAT-100 데이타셋\n",
    "- 총 50,000개의 학습 데이타, 10,000개의 테스트 데이터\n",
    "- 이미지 - 32X32X3 \n",
    "- MNIST 데이터 셋에 비해 데이터 복잡도가 훨씬 높아 높은 성능을 기대하기는 어렵다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/cifar-10.jpg\" align=left width=400 height=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CIFAR-10 데이타셋을 이용한 CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) CIFAR-10 데이터 다운로드 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# 데이터를 다운받습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) CIFAR-10 데이터 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat',\n",
    "              'deer', 'dog', 'frog', 'horse',\n",
    "              'ship', 'truck']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 데이타 셋 전처리 \n",
    "- 데이타 셋의 평균과 표준편차를 채널별로 구해 표준화 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평균과 표준편차는 채널별로 구해줍니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 검증 데이타 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) CNN 모델 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) 모델 컴파일 구성\n",
    "- 레이블 형태를 원-핫 인코딩을 이용하여 범주형 형태로 변환했을 때 사용하는 손실 함수 ->  'categorical_crossentropy' 사용\n",
    "- 레이블 형태를 원-핫 인코딩 하지 않고 레이블 형태(0~9) 그대로 사용할 때 사용하는 손실함수 -> 'sparse_categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) 학습 과정 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 과대적합의 문제 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) 신경망 시각화해보기\n",
    "- 모델이 학습한 특징을 시각화하는 것은 모델을 해석하는데 도움이 될 수 있다.\n",
    "- CIFAR-10 데이타셋의 두번째 테스트 데이타(배)를 시각화 -> 배의 특징맵 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "get_layer_name = [layer.name for layer in model.layers]\n",
    "get_output = [layer.output for layer in model.layers]\n",
    "\n",
    "# 모델 전체에서 output을 가져올 수 있습니다.\n",
    "visual_model = tf.keras.models.Model(inputs = model.input, outputs = get_output)\n",
    "\n",
    "# 테스트셋의 두 번째 데이터는 '배'입니다.\n",
    "test_img = np.expand_dims(x_test[1], axis = 0)\n",
    "feature_maps = visual_model.predict(test_img)\n",
    "\n",
    "for layer_name, feature_map in zip(get_layer_name, feature_maps):\n",
    "    # Dense 층은 제외합니다.\n",
    "    if(len(feature_map.shape) == 4):\n",
    "        img_size = feature_map.shape[1]\n",
    "        features = feature_map.shape[-1]\n",
    "        \n",
    "        # (img_size, img_size)의 feature_map이 features 개수만큼 존재합니다.\n",
    "        display_grid = np.zeros((img_size, img_size * features))\n",
    "        \n",
    "        # 각 특징맵을 display_grid 배열에 이어붙입니다.\n",
    "        for i in range(features):\n",
    "            x = feature_map[0, :, :, i]\n",
    "            x -= x.mean(); x /= x.std()\n",
    "            x *= 64; x += 128\n",
    "            x = np.clip(x, 0, 255).astype('uint8')\n",
    "            display_grid[:, i * img_size : (i + 1) * img_size] = x\n",
    "            \n",
    "        plt.figure(figsize = (features, 2 + 1./features))\n",
    "        plt.title(layer_name, fontsize = 20)\n",
    "        plt.grid(False)\n",
    "        plt.imshow(display_grid, aspect = 'auto', cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 과대적합 피하기\n",
    "- 과대적합을 피하기 위한 가장 좋은 방법은 데이터를 충분히 모아놓고 모델을 구현하는 것\n",
    "\n",
    "### 과대 적합 피하는 방법\n",
    "#### (1) 규제화 함수(regulaizer) 사용하기\n",
    "- 규제화 함수(regulaizer : 임의로 모델의 복잡도를 제한시키는 것을 의미\n",
    "- L1 노름(라쏘, LASSO), L2 노름(릿지, Ridge), L1 노름과 L2 노름을 혼합한 엘라스틱넷(ElasticNet)\n",
    "- tensorflow, keras, regularizers, l1(l=0.01) - 가중치의 절대값 합\n",
    "- tensorflow, keras, regularizers, l2(l=0.01) - 가중치의 제곱합\n",
    "- tensorflow, keras, regularizers, l1_l2(i1-0.01, l2=0.01) - (혼합) 절대값의 합 + 제곱합\n",
    "- 각 규제화 함수는 기능에 맞게 가중치의 합을 구하여 손실함수에 더하게 된다.\n",
    "- (예) l2 규제화 함수 = 가중치의 제곱합 + 손실 \n",
    "\n",
    "#### (2) 드롭 아웃(drop out) \n",
    "- 과대 적합을 피하기 위해 사용되는 대표적인 방법 중 하나\n",
    "- 학습이 진행되는 동안 신경망의 일부 유닛을 제외(드롭) 한다.\n",
    "- 신경망 모델은 드룹 아웃으로 제외한 유닛 대신 제외하지 않은 유닛을 집중적으로 학습\n",
    "- 과대 적합 문제를 방지함과 동시에 더 나은 성능을 기대할 수 있다. -> 선택과 집중\n",
    "- 드롭 아웃 비율(dropout rate)는 일반적으로 0.2 ~ 0.5를 사용\n",
    "- 드롭아웃은 학습 속도를 느리게 하는 단점이 존재\n",
    "- 테스트 시에는 드룹아웃이 작동하지 않으며, 모든 유닛이 활성화되어 출력하게 된다.\n",
    "\n",
    "#### (3) 배치 정규화(Batch Normalization)\n",
    "- 근본적으로 과대적합을 피하기 위한 방법은 아님\n",
    "- 배치 정규화와 드롭아웃과 배교하여 설명\n",
    "- 내부 공선성을 해결하기 위해 고안\n",
    "- 신경망의 출력값은 다양한 입력 데이타에 따라 쉽게 변할 수 있는데, \n",
    "- 매우 큰 범위의 출력값은 신경망을 불안정하게 하여 성능저하를 일으킬 수 있다.\n",
    "- 배치 정규화는 출력값이 가질 수 있는 범위,즉 출력값 분포의 범위를 줄여주어 불확실성을 어느 정도 감소시키는 방법\n",
    "- 배치 정규화의 장점\n",
    "    - 1) 기존 신경망은 높은 학습률을 사용하는 경우, 그래디언트 손실/폭발의 문제점이 존재\n",
    "        - 배치 정규화를 사용하면, 이러한 문제를 방지할 수 있어 높은 학습률을 사용하여 빠른 속도로 학습을 진행할 있게 한다.\n",
    "    - 2) 배치 정규화는 자체적인 규제 효과가 있기 때문에, 과대적합 문제를 피할 수 있게 한다.\n",
    "        - 과대적합에 도움이 될 뿐 보장하지는 않는다. \n",
    "        - \"배치정규화를 사용하면 별도의 규제화 함수나 드롭아웃을 사용하지 않아도 된다\"라는 의견이 다수    \n",
    "- 배치 정규화를 사용할 경우, 일반적으로 아래와 같은 순서를 이용하여 모델을 구성\n",
    "    - Dense층 또는 Conv2D층 -> BatchNormalization() -> Activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 규제화 함수 사용\n",
    "- l2 규제화 함수 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 규제화 함수를 사용하지 않은 모델에 비해 비교적 그래프가 안정적으로 그려지는 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 드롭 아웃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 드롭아웃을 이용한 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 두 그래프의 선이 벌어지지 않고 있다.\n",
    "- 과대적합을 해결할 방법을 사용하지 않은 모델과 규제화 함수를 사용한 모델과 비교했을 때 \n",
    "- 과대적합을 방지하기에 드롭아웃 방법은 매우 강력해 보임\n",
    "- 드롭아웃은 학습 속도를 느리게 하는 단점이 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 배치 정규화\n",
    "- 배치 정규화를 사용할 경우, 일반적으로 아래와 같은 순서를 이용하여 모델을 구성\n",
    "    - Dense층 또는 Conv2D층 -> BatchNormalization() -> Activation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 배치 정규화 이용한 모델 구성 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 학습 과정 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프 설명)\n",
    "- 10 에폭 이후로 과대 적합이 발생하고 있지만, 가장 높은 성능을 달성했다는 점에서 이를 보완 \n",
    "- 배치 정규화는 다수의 모델에서 사용되고 있기 때문에 자주 접하고 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이타 증식 사용하기\n",
    "\n",
    "### 데이타 증식(Data Augmentation)\n",
    "- 기존 데이타에 적절한 변형을 추가하여 새로운 데이터를 만들어내는 방법\n",
    "- 딥러닝의 고질적인 문제인 일반화(Generalization) 문제의 대응책, 근본적으로는 해결을 못함\n",
    "- 데이타 증식의 장점\n",
    "    - 1) 다양한 데이터를 입력시킴으로써 모델을 더욱 견고하게 만들어주기 때문에 테스트 시에 더 높은 성능을 기대할 수 있다.\n",
    "    - 2) 수집된 데이타가 적은 경우에 강력함 힘을 발휘한다.\n",
    "- 데이타가 적은 경우 모델을 일반화 시키기 어려움\n",
    "    - 테스트 시에 성능이 저하될 우려가 높고, 과대적합이 발생할 확률이 매우 높다.\n",
    "    - 데이타 증식을 이용하면 모델 성능에 좋은 효과가 있음\n",
    "- 훈련(train) data만 이미지 증식을 사용, 검증 데이타와 테스트 데이타는 이미지 증식을 시키지 않음\n",
    "\n",
    "#### 케라스 이미지 제너레이션(Image Generator)가 제공하는 변환 방식\n",
    "- width_shift_range : 임의의 크기만큼 너비 방향으로 이동시킨다.\n",
    "    - 0.2이고 이미지의 너비가 100이라면 ->-20~+20의 범위에서 너비 방향으로 이동시킨다.\n",
    "- height_shift_range : 임의의 크기만큼 높이 방향으로 이동시킨다.\n",
    "    - 0.2이고 이미지의 높이가 100이라면 ->-20~+20의 범위에서 높이 방향으로 이동시킨다. \n",
    "- brightness_range : 이미지의 밝기 정도를 조정한다.\n",
    "    - (0.5, 1.5)이면 원본 대비 최대 50%의 비율로 어둡거나 밝게 조절한다.\n",
    "- shear_range : 시계 반대 방향으로 밀림 강도를 조절한다.\n",
    "    - 0.5 이면 최대 50%의 비율로 시계반대방향으로 기울어지게 된다.\n",
    "- zoome_range : 임의의 비율만큼 이미지를 확대/축소시킨다.\n",
    "    - 0.5이면, 0.5~1.5배의 범위에서 이미지의 크기를 조절한다.\n",
    "- rotation_range : 이미지를 임의로 회전시킨다.\n",
    "    - 180이라면, 0 ~ 180의 범위에서 임의로 이미지를 회전시킨다.\n",
    "- rescale : 이미지 픽셀값의 크기를 조절한다.\n",
    "    - 1/255이라면, 각 픽셀값에 해당 값이 곱해진다.\n",
    "- fill_mode : 이미지 변환 시에 새로 생기는 픽셀을 채울 방법을 결정한다.\n",
    "    - [\"nearest\", \"constant\", \"reflect or wrap\"]\n",
    "- horizontal_fip : True인 경우, 임의로 이미지를 수평 방향으로 뒤집는다.\n",
    "- vertical_fip : True인 경우, 임의로 이미지를 수직 방향으로 뒤집는다.\n",
    "- processing_function : 사용자 정의 전처리 함수 또는 전처리 함수를 적용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 이미지 제네레이터를 사용하여 이미지 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 데이타 불러오기/데이타 표준화/검증데이타 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# 평균과 표준편차는 채널별로 구해줍니다.\n",
    "x_mean = np.mean(x_train, axis = (0, 1, 2))\n",
    "x_std = np.std(x_train, axis = (0, 1, 2))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, \n",
    "                                                  test_size = 0.3, random_state = 777)\n",
    "\n",
    "# sparse_categorical_crossentropy를 사용하기 위해선 (batch_size, ) 형태를 가져야합니다.\n",
    "# 따라서 np.squeeze를 사용해서 마지막 차원을 없애줍니다.\n",
    "# y_train의 경우 (35000, 1) -> (35000, )이 됩니다.\n",
    "y_train = np.squeeze(y_train)\n",
    "y_val = np.squeeze(y_val)\n",
    "\n",
    "print('data ready~')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 이미지 제네레이터를 사용하여 이미지 증식하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 이미지 제네레이터를 사용하여 증식된 데이타 모델 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.fit() 함수의 steps_per_epoch 인수\n",
    "    - 1 에폭에 배치 크기만큼의 데이터를 몇 번 전달해줄지 결정하는 인수\n",
    "    - 학습 데이타의 개수/배치 크기 = 35000/32 = 1094번\n",
    "    - 1094번의 스템을 통해 전체 데이타를 사용하게 됨\n",
    "    - 만약 1094보다 작은 수를 전달할 경우, 전체 데이타를 사용하지 않으므로 주의해야 함\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 학습 과정 그려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "his_dict = history.history\n",
    "loss = his_dict['loss']\n",
    "val_loss = his_dict['val_loss'] \n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "\n",
    "# 훈련 및 검증 손실 그리기\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(epochs, loss, color = 'blue', label = 'train_loss')\n",
    "ax1.plot(epochs, val_loss, color = 'orange', label = 'val_loss')\n",
    "ax1.set_title('train and val loss')\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('loss')\n",
    "ax1.legend()\n",
    "\n",
    "acc = his_dict['acc']\n",
    "val_acc = his_dict['val_acc']\n",
    "\n",
    "# 훈련 및 검증 정확도 그리기\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(epochs, acc, color = 'blue', label = 'train_acc')\n",
    "ax2.plot(epochs, val_acc, color = 'orange', label = 'val_acc')\n",
    "ax2.set_title('train and val acc')\n",
    "ax2.set_xlabel('epochs')\n",
    "ax2.set_ylabel('acc')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
