{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝-회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보스톤 집값 예측 데이타 셋 : ./dataset/housing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/boston.png\" align=left width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1978년, 집값에 가장 큰 영향을 미치는 것이 ‘깨끗한 공기’라는 연구 결과가 \n",
    "     - 하버드대학교 도시개발학과에서 발표됨\n",
    "- 이들은 자신의 주장을 뒷받침하기 위해 집값의 변동에 영향을 미치는 여러 가지 요인을 모아서 \n",
    "     - 환경과 집값의 변동을 보여주는 데이터셋을 만듦\n",
    "- 이 데이타가 현재 선형 회귀를 테스트하는 가장 유명한 데이터로 쓰이고 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 선형 회귀(Linear Regression) \n",
    "- 분류(Classfification) \n",
    "     - 참(1) 또는 거짓(0)을 맞히는 문제, 여러 개의 보기 중 맞는 하나를 예측하는 문제\n",
    "- 선형 회귀(Linear Regression)\n",
    "     - 하나의 정답을 맞히는 것이 아니라 수치를 예측하는 문제\n",
    "     - 주어진 환경 요인과 집값의 변동을 학습해서 환경 요인만 놓고 집값을 예측하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이타 불러오기\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./dataset/housing.csv\", delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이타 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 14)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0.06263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.593</td>\n",
       "      <td>69.1</td>\n",
       "      <td>2.4786</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>391.99</td>\n",
       "      <td>9.67</td>\n",
       "      <td>22.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0.04527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.120</td>\n",
       "      <td>76.7</td>\n",
       "      <td>2.2875</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.08</td>\n",
       "      <td>20.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0.06076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.976</td>\n",
       "      <td>91.0</td>\n",
       "      <td>2.1675</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.64</td>\n",
       "      <td>23.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0.10959</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.794</td>\n",
       "      <td>89.3</td>\n",
       "      <td>2.3889</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>393.45</td>\n",
       "      <td>6.48</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0.04741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.93</td>\n",
       "      <td>0</td>\n",
       "      <td>0.573</td>\n",
       "      <td>6.030</td>\n",
       "      <td>80.8</td>\n",
       "      <td>2.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>273.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>396.90</td>\n",
       "      <td>7.88</td>\n",
       "      <td>11.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0    1      2   3      4      5     6       7   8      9     10  \\\n",
       "501  0.06263  0.0  11.93   0  0.573  6.593  69.1  2.4786   1  273.0  21.0   \n",
       "502  0.04527  0.0  11.93   0  0.573  6.120  76.7  2.2875   1  273.0  21.0   \n",
       "503  0.06076  0.0  11.93   0  0.573  6.976  91.0  2.1675   1  273.0  21.0   \n",
       "504  0.10959  0.0  11.93   0  0.573  6.794  89.3  2.3889   1  273.0  21.0   \n",
       "505  0.04741  0.0  11.93   0  0.573  6.030  80.8  2.5050   1  273.0  21.0   \n",
       "\n",
       "         11    12    13  \n",
       "501  391.99  9.67  22.4  \n",
       "502  396.90  9.08  20.6  \n",
       "503  396.90  5.64  23.9  \n",
       "504  393.45  6.48  22.0  \n",
       "505  396.90  7.88  11.9  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/boston2.png\" align=left width=650>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 총 샘플의 수는 506개이고, 컬럼 수는 14개 - 13개의 속성과 1개의 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.613524</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "      <td>22.532806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.601545</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "      <td>9.197104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "      <td>17.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "      <td>21.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.677082</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.613524   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.601545   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.677082   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12          13  \n",
       "count  506.000000  506.000000  \n",
       "mean    12.653063   22.532806  \n",
       "std      7.141062    9.197104  \n",
       "min      1.730000    5.000000  \n",
       "25%      6.950000   17.025000  \n",
       "50%     11.360000   21.200000  \n",
       "75%     16.955000   25.000000  \n",
       "max     37.970000   50.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 속성과 클래스 구분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:, 0:13]  # 속성\n",
    "Y = dataset[:, 13]    # 클래스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 데이타와 테스트 데이타 나누기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354, 13), (152, 13))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((354,), (152,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "# seed 값 설정\n",
    "seed = 0\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# 모델 설정\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=13, activation='relu'))  # 입력층 - 13개 노드, 은닉층1 - 30개 노드\n",
    "model.add(Dense(6, activation='relu'))                 # 은닉층2 - 6개 노드\n",
    "model.add(Dense(1))                                    # 출력층 - 1개 노드, 활성화 함수 사용하지 않음(수치 출력)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 8438.1162\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 732.8683\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 512.5950\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 388.6990\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 256.9753\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 170.7661\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 135.6643\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 123.0020\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 92.7171\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 80.1417\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 72.3035\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 69.8260\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 67.6638\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 65.8864\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 65.1428\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 63.5601\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 64.5949\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 61.6633\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 62.5243\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 59.6989\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 59.3723\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 58.5467\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 58.7318\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.2302\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.2374\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 56.2391\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 54.9049\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 54.9681\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 54.0103\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 53.4721\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 52.6120\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 52.3308\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 51.6922\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 51.4115\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.9594\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.0806\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 49.6587\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 50.1437\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.7939\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.2098\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 48.1594\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 47.6336\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.4014\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.0603\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.2956\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 46.1978\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 44.8460\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 43.1158\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.7315\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 42.7363\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.1165\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 41.3375\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 40.9050\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.9033\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.7777\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - ETA: 0s - loss: 25.75 - 0s 1ms/step - loss: 39.4455\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 39.0699\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.7630\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.5026\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.5739\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 38.2261\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.1457\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 36.0968\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 34.8202\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.8550\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 35.0674\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.6298\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.4776\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 37.0616\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 33.4909\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.8610\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 33.0652\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.9464\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.9932\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 32.2534\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.4631\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.7118\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.5142\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.4484\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.6032\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.3145\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.3669\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 31.6188\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.0275\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.3375\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.4044\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.4635\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.5894\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.4563\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.0863\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 28.2004\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.7010\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.8506\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 30.4518\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.2899\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.6983\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.7705\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.6901\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3404\n",
      "Epoch 100/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3621\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.5198\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.4034\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 27.0355\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 29.2855\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3013\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3194\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.9012\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.4835\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.7045\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.2438\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.0588\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.9572\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.1499\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 25.3550\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.2317\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.9471\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.0546\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1469\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.7542\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.8985\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.4217\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.0718\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.9580\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.8591\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1669\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.6643\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.3838\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.6999\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.6268\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.9588\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.2078\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.1276\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.4127\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 24.1749\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.8195\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.3742\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.4675\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.5046\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.8010\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.1629\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.0378\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.3135\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.3823\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.0798\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 26.9651\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.7749\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.0281\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.8411\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.4579\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.5281\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 23.3860\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.1350\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.1964\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.7092\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.3455\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 22.0297\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.7219\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.2497\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.4643\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.2341\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.8845\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.1380\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.3340\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 21.2743\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.6588\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.0757\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.4992\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.1761\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.7556\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 18.2176\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.2940\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.3091\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.2523\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.5519\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.9460\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.8371\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 997us/step - loss: 18.6557\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.4369\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.7587\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.7811\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.0760\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.1717\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.8656\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 20.4789\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.9117\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.0737\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.0016\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.8635\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.3195\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.0059\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.2478\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.8926\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.4692\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 18.4384\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 16.6829\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.5807\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.1213\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36/36 [==============================] - 0s 1ms/step - loss: 17.6453\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 19.6160\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 17.8496\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x265f8e8d2c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예측값과 실제값 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.365078 ],\n",
       "       [25.240007 ],\n",
       "       [24.272375 ],\n",
       "       [12.595086 ],\n",
       "       [18.163317 ],\n",
       "       [20.790972 ],\n",
       "       [20.769337 ],\n",
       "       [22.056067 ],\n",
       "       [16.585564 ],\n",
       "       [ 7.744078 ],\n",
       "       [ 8.018615 ],\n",
       "       [11.964857 ],\n",
       "       [13.730315 ],\n",
       "       [12.351804 ],\n",
       "       [46.15092  ],\n",
       "       [29.792612 ],\n",
       "       [22.4222   ],\n",
       "       [34.383698 ],\n",
       "       [29.374914 ],\n",
       "       [21.210531 ],\n",
       "       [23.45303  ],\n",
       "       [18.868534 ],\n",
       "       [18.710537 ],\n",
       "       [25.158276 ],\n",
       "       [21.387268 ],\n",
       "       [13.982561 ],\n",
       "       [20.381248 ],\n",
       "       [10.212119 ],\n",
       "       [34.788647 ],\n",
       "       [18.391521 ],\n",
       "       [14.045837 ],\n",
       "       [17.898876 ],\n",
       "       [19.51824  ],\n",
       "       [20.717997 ],\n",
       "       [23.350594 ],\n",
       "       [20.967731 ],\n",
       "       [12.411643 ],\n",
       "       [27.161783 ],\n",
       "       [12.4477005],\n",
       "       [13.129853 ],\n",
       "       [23.492252 ],\n",
       "       [20.770657 ],\n",
       "       [19.77178  ],\n",
       "       [13.874795 ],\n",
       "       [24.256111 ],\n",
       "       [22.33689  ],\n",
       "       [20.378794 ],\n",
       "       [15.42738  ],\n",
       "       [16.717146 ],\n",
       "       [22.890318 ],\n",
       "       [ 8.25167  ],\n",
       "       [17.330204 ],\n",
       "       [19.700232 ],\n",
       "       [30.81064  ],\n",
       "       [10.334753 ],\n",
       "       [19.262093 ],\n",
       "       [19.65552  ],\n",
       "       [17.351774 ],\n",
       "       [10.252768 ],\n",
       "       [20.825317 ],\n",
       "       [21.237097 ],\n",
       "       [20.442774 ],\n",
       "       [31.253801 ],\n",
       "       [31.167025 ],\n",
       "       [14.753836 ],\n",
       "       [33.342808 ],\n",
       "       [15.617408 ],\n",
       "       [19.803417 ],\n",
       "       [11.811082 ],\n",
       "       [20.524776 ],\n",
       "       [17.787933 ],\n",
       "       [23.480024 ],\n",
       "       [30.883783 ],\n",
       "       [30.552382 ],\n",
       "       [28.71728  ],\n",
       "       [12.286291 ],\n",
       "       [32.52731  ],\n",
       "       [21.021568 ],\n",
       "       [21.68697  ],\n",
       "       [18.97587  ],\n",
       "       [23.745247 ],\n",
       "       [18.35167  ],\n",
       "       [19.361967 ],\n",
       "       [34.371227 ],\n",
       "       [33.97701  ],\n",
       "       [24.514011 ],\n",
       "       [21.036676 ],\n",
       "       [12.433846 ],\n",
       "       [30.015673 ],\n",
       "       [14.347021 ],\n",
       "       [16.716236 ],\n",
       "       [11.850124 ],\n",
       "       [23.951801 ],\n",
       "       [28.792978 ],\n",
       "       [22.528793 ],\n",
       "       [19.68806  ],\n",
       "       [12.464368 ],\n",
       "       [25.158587 ],\n",
       "       [13.754068 ],\n",
       "       [20.796986 ],\n",
       "       [20.275824 ],\n",
       "       [19.783329 ],\n",
       "       [25.58269  ],\n",
       "       [18.733562 ],\n",
       "       [20.525272 ],\n",
       "       [20.694239 ],\n",
       "       [10.479942 ],\n",
       "       [17.583696 ],\n",
       "       [21.73938  ],\n",
       "       [23.399126 ],\n",
       "       [35.340477 ],\n",
       "       [13.015065 ],\n",
       "       [20.323727 ],\n",
       "       [14.30982  ],\n",
       "       [17.916662 ],\n",
       "       [19.836256 ],\n",
       "       [11.329533 ],\n",
       "       [21.801775 ],\n",
       "       [13.734342 ],\n",
       "       [48.36585  ],\n",
       "       [32.51304  ],\n",
       "       [14.238913 ],\n",
       "       [20.942665 ],\n",
       "       [19.082977 ],\n",
       "       [21.238676 ],\n",
       "       [20.808344 ],\n",
       "       [36.860943 ],\n",
       "       [18.817852 ],\n",
       "       [21.181213 ],\n",
       "       [29.971743 ],\n",
       "       [15.730463 ],\n",
       "       [12.103005 ],\n",
       "       [17.894182 ],\n",
       "       [12.316656 ],\n",
       "       [11.777084 ],\n",
       "       [33.04716  ],\n",
       "       [19.618753 ],\n",
       "       [17.333082 ],\n",
       "       [24.135134 ],\n",
       "       [11.540671 ],\n",
       "       [12.589924 ],\n",
       "       [18.503313 ],\n",
       "       [38.50474  ],\n",
       "       [26.86511  ],\n",
       "       [26.781633 ],\n",
       "       [19.210049 ],\n",
       "       [31.53713  ],\n",
       "       [29.358135 ],\n",
       "       [12.100906 ],\n",
       "       [12.898663 ],\n",
       "       [30.58575  ],\n",
       "       [28.569628 ]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예측값\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).shape # 2차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test).flatten().shape # 1차원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격: 22.600, 예상가격: 23.365, 잔차-0.765\n",
      "실제가격: 50.000, 예상가격: 25.240, 잔차24.760\n",
      "실제가격: 23.000, 예상가격: 24.272, 잔차-1.272\n",
      "실제가격: 8.300, 예상가격: 12.595, 잔차-4.295\n",
      "실제가격: 21.200, 예상가격: 18.163, 잔차3.037\n",
      "실제가격: 19.900, 예상가격: 20.791, 잔차-0.891\n",
      "실제가격: 20.600, 예상가격: 20.769, 잔차-0.169\n",
      "실제가격: 18.700, 예상가격: 22.056, 잔차-3.356\n",
      "실제가격: 16.100, 예상가격: 16.586, 잔차-0.486\n",
      "실제가격: 18.600, 예상가격: 7.744, 잔차10.856\n"
     ]
    }
   ],
   "source": [
    "# 예측 값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten() # 1차원으로 변경\n",
    "\n",
    "for i in range(10):\n",
    "    label = Y_test[i] # 실제값\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}, 잔차{:.3f}\".format(label, prediction, label-prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
